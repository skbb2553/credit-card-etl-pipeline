import numpy as np
import pandas as pd
import os
import shutil
import random
import refine  


# ... å…¶ä»– import


try:
    from Himitsu import CUSTOM_CARD_MAP
    print("ğŸ” å·²è¼‰å…¥ Himitsu.py æŒ‡å®šçš„å¡è™Ÿæ˜ å°„ã€‚")
except ImportError:
    print("âš ï¸ æ‰¾ä¸åˆ° Himitsu.pyï¼Œå°‡ä½¿ç”¨å…¨è‡ªå‹•éš¨æ©Ÿå¡è™Ÿã€‚")
    CUSTOM_CARD_MAP = {} # è‹¥æ²’æœ‰æª”æ¡ˆï¼Œå°±ä¿æŒç©ºå­—å…¸ï¼Œç¨‹å¼æœƒèµ°è‡ªå‹•éå¢é‚è¼¯


# å°šæœªå®šç¾©çš„å¡è™Ÿï¼Œæœƒå¾é€™å€‹æ•¸å­—é–‹å§‹è‡ªå‹•éå¢
AUTO_INCREMENT_START = 1000


# ==========================================
# é…ç½®å€
# ==========================================
SOURCE_FILE = 'data/result_all_banks.csv'
OUTPUT_DIR = 'examples'  # è¼¸å‡ºåˆ° examples è³‡æ–™å¤¾ä¾› GitHub ä½¿ç”¨
SAMPLE_SIZE = 30         # ç¯„æœ¬è¦å¹¾ç­†è³‡æ–™

# ç‚ºäº†å±•ç¤º Regex èƒ½åŠ›ï¼Œæˆ‘å€‘å¸Œæœ›ä¿ç•™ç‰¹å®šçš„é«’è³‡æ–™
# é€™è£¡å¯ä»¥ç”¨é—œéµå­—å¼·åˆ¶ä¿ç•™æŸäº›æœ‰è¶£çš„æ¡ˆä¾‹
INTERESTING_KEYWORDS = [
    'é€£åŠ ', 'Line', 'çµ±ä¸€è¶…å•†', 'å…¨å®¶', 'UBER', 'NETFLIX', 
    'Steam', 'è¦çš®', 'foodpanda', 'ç¹³æ¬¾'
]


def ensure_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)

# ==========================================
# 1. æ™ºæ…§æ¡æ¨£ (Smart Sampling)
# ==========================================
def smart_sample(df, n=20):
    """
    ä¸åªæ˜¯éš¨æ©Ÿï¼Œè€Œæ˜¯å„ªå…ˆæŒ‘é¸ã€Œçœ‹èµ·ä¾†å¾ˆé«’ã€æˆ–ã€Œæœ‰ä»£è¡¨æ€§ã€çš„è³‡æ–™
    """
    pool = []
    
    # 1. é—œéµå­—å‘½ä¸­æ¡æ¨£ (ç¢ºä¿å±•ç¤ºæ¡ˆä¾‹åŒ…å«å„ç¨®æ”¯ä»˜å ´æ™¯)
    for kw in INTERESTING_KEYWORDS:
        mask = df['Merchant'].astype(str).str.contains(kw, case=False, na=False)
        if mask.any():
            # æ¯å€‹é—œéµå­—æŠ½ 1-2 ç­†
            sample = df[mask].sample(min(len(df[mask]), 2))
            pool.append(sample)
    
    # 2. éš¨æ©Ÿè£œè¶³å‰©é¤˜æ•¸é‡
    current_count = sum([len(x) for x in pool])
    if current_count < n:
        remaining = n - current_count
        pool.append(df.sample(remaining))
        
    sampled_df = pd.concat(pool).drop_duplicates().reset_index(drop=True)
    return sampled_df

# ==========================================
# 2. å»æ•å¼•æ“ (Masking Engine)
# ==========================================
def anonymize_data(df):
    """
    åŸ·è¡Œéå°ç¨±å»æ•ï¼šä¿ç•™æ ¼å¼çœŸå¯¦æ€§ï¼Œä½†æ•¸å€¼èˆ‡å€‹è³‡é€ å‡
    """
    print(">>> åŸ·è¡Œå»æ•åŒ–è™•ç†...")
    
    # A. æ—¥æœŸå¹³ç§» (å…¨éƒ¨ç§»åˆ° 2023 å¹´ï¼Œä¿æŒç›¸å°é–“éš”)
    # æ‰¾å‡ºè³‡æ–™ä¸­çš„æœ€å¤§æ—¥æœŸï¼Œç®—å‡ºèˆ‡ 2023-12-31 çš„å·®å€¼ï¼Œé€²è¡Œå…¨é«”å¹³ç§»
    if 'Transaction_Date' in df.columns:
        dates = pd.to_datetime(df['Transaction_Date'], errors='coerce')
        valid_dates = dates.dropna()
        if not valid_dates.empty:
            max_date = valid_dates.max()
            # è®“æœ€æ–°ä¸€ç­†è³‡æ–™è®Šæˆ '2023-12-25' (è™›æ§‹éå»æ™‚é–“)
            target_date = pd.to_datetime('2023-12-25')
            delta = target_date - max_date
            
            # å¥—ç”¨å¹³ç§»
            for col in ['Transaction_Date', 'Posting_Date', 'Conversion_Date']:
                if col in df.columns:
                    # è½‰ datetime -> å¹³ç§» -> è½‰å›å­—ä¸² (YYYY-MM-DD)
                    dt_series = pd.to_datetime(df[col], errors='coerce')
                    df[col] = (dt_series + delta).dt.strftime('%Y-%m-%d')
    
    # B. é‡‘é¡æ“¾å‹• (Noise Injection)
    # é‚è¼¯ï¼šé‡‘é¡ * (0.9 ~ 1.1 çš„éš¨æ©Ÿæ•¸)ï¼Œä¸¦å–æ•´æ•¸æˆ–å°æ•¸é»å¾Œå…©ä½
    numeric_cols = ['Amount', 'Currency_Amount', 'Payment_Amount']
    for col in numeric_cols:
        if col in df.columns:
            # ç”¢ç”Ÿéš¨æ©Ÿé›œè¨Š mask (e.g., 0.95 ~ 1.05)
            noise = np.random.uniform(0.95, 1.05, size=len(df))
            
            # è™•ç†åŸå§‹æ•¸æ“š (é˜²å‘†ï¼šè½‰æ•¸å€¼)
            val = pd.to_numeric(df[col], errors='coerce').fillna(0)
            
            # æ“¾å‹•å¾Œï¼Œè‹¥æ˜¯å°å¹£é€šå¸¸ç‚ºæ•´æ•¸ï¼Œå¤–å¹£ä¿ç•™å°æ•¸
            if 'Currency' in col or 'Amount' in col: 
                # ç°¡å–®åˆ¤æ–·ï¼šè‹¥åŸæ¬„ä½çœ‹èµ·ä¾†åƒæ•´æ•¸ï¼Œå°±è½‰æ•´æ•¸
                is_integer_col = (val % 1 == 0).all()
                new_val = val * noise
                if is_integer_col:
                    df[col] = new_val.round(0).astype(int)
                else:
                    df[col] = new_val.round(2)

# C. æ•æ„Ÿå€‹è³‡è¦†è“‹ (Masking) - æ”¯æ´æŒ‡å®šå¡è™Ÿ
    if 'Card_No' in df.columns:
        # 1. å–å¾—è³‡æ–™ä¸­æ‰€æœ‰çš„çœŸå¯¦å¡è™Ÿ (Unique)
        # æ³¨æ„ï¼šæœ‰äº›è³‡æ–™å¯èƒ½æ˜¯ None/NaNï¼Œè¦æ¿¾æ‰
        real_cards = df['Card_No'].dropna().unique()
        
        fake_map = {}
        auto_counter = 0
        
        for real_card in real_cards:
            real_card_str = str(real_card).strip()
            # å˜—è©¦å¾çœŸå¯¦å¡è™Ÿä¸­æå–æœ«å››ç¢¼ (å‡è¨­æ ¼å¼å¯èƒ½åŒ…å« - æˆ– *)
            # é€™è£¡å‡è¨­ etl.py ç”¢å‡ºçš„ Card_No å·²ç¶“æ˜¯ä¹¾æ·¨çš„ '1234' æˆ– '****1234'
            # æˆ‘å€‘ç›´æ¥å–æœ€å¾Œ 4 ç¢¼ä½œç‚º Key
            real_suffix = real_card_str[-4:]
            
            # [æ ¸å¿ƒé‚è¼¯] åˆ¤æ–·æ˜¯å¦åœ¨ä½¿ç”¨è€…å®šç¾©çš„æ¸…å–®ä¸­
            if real_suffix in CUSTOM_CARD_MAP:
                target_suffix = CUSTOM_CARD_MAP[real_suffix]
            else:
                # è‹¥æ²’å®šç¾©ï¼Œå°±è‡ªå‹•ç”¢ç”Ÿ (ä¾‹å¦‚ 1000, 2000...)
                auto_counter += 1
                target_suffix = str(AUTO_INCREMENT_START * auto_counter)
            
            # å»ºç«‹å®Œæ•´çš„å‡å¡è™Ÿå­—ä¸²
            fake_map[real_card] = f"****-****-****-{target_suffix}"
            
        # å¥—ç”¨ Mapping
        df['Card_No'] = df['Card_No'].map(fake_map).fillna(df['Card_No'])
        
        # [é‡è¦] é€™è£¡è¦å›å‚³ fake_mapï¼Œå› ç‚º generate_dummy_configs éœ€è¦ç”¨å®ƒä¾†å¯«å…¥ CSV
        return df, fake_map

    return df, {}

# ==========================================
# 3. è¨­å®šæª”æ·¨åŒ– (Config Sanitizer)
# ==========================================
def generate_dummy_configs(real_config_dir, output_config_dir, card_map):
    print(f">>> ç”Ÿæˆç¯„ä¾‹è¨­å®šæª”è‡³ {output_config_dir}...")
    ensure_dir(output_config_dir)
    
    # --- [æ–°å¢] å°ç£ç†±é–€ç¥å¡æ±  (Flavor Text) ---
    # è®“ Mock Data çœ‹èµ·ä¾†åƒçœŸçš„ä¸€æ¨£ï¼ŒåŒ…å«å¸¸è¦‹çš„è¡Œå‹•æ”¯ä»˜è¨­å®š
    POPULAR_CARDS_POOL = [
        {'name': 'åœ‹æ³°CUBEå¡', 'mobile': 'ApplePay', 'prefix': 'ApplePayï¼', 'note': 'æ¬Šç›Šåˆ‡æ›å¡'},
        {'name': 'ç‰å±±Unicard', 'mobile': 'LinePay', 'prefix': 'LinePayï¼', 'note': 'è‡ªé¸æ¬Šç›Šå¡'},
        {'name': 'ç‰å±±Ubearå¡', 'mobile': '', 'prefix': '', 'note': 'ç„¡è…¦ç¶²è³¼å¡'},
        {'name': 'å°æ–°Richartå¡', 'mobile': 'SamsungPay', 'prefix': 'SamsungPayï¼', 'note': 'ä¸ƒé¸ä¸€ç„¡ä¸Šé™å›é¥‹'},
        {'name': 'å¯Œé‚¦Jå¡', 'mobile': '', 'prefix': '', 'note': 'æ—¥éŸ“æ—…éŠå¡'},
        {'name': 'è¯é‚¦å‰é¶´å¡', 'mobile': 'ApplePay', 'prefix': 'ApplePayï¼', 'note': 'æ—¥éŸ“æ—…éŠå¡'},
        {'name': 'æ°¸è±DAWHOç¾é‡‘å›é¥‹ä¿¡ç”¨å¡', 'mobile': 'GooglePay', 'prefix': 'GooglePayï¼', 'note': 'ç¾é‡‘ç„¡è…¦å›é¥‹å¡'},
        {'name': 'ä¸­ä¿¡Uniopenè¯åå¡', 'mobile': '', 'prefix': '', 'note': 'çµ±ä¸€é›†åœ˜ç”Ÿæ…‹åœˆè¯åå¡'},
        {'name': 'ä¸­ä¿¡LINE Payä¿¡ç”¨å¡', 'mobile': 'LinePay', 'prefix': 'LinePayï¼', 'note': 'LinePayè¯åå¡'},
    ]
    
    # 1. è™•ç† Card Mapping
    real_card_map_path = os.path.join(real_config_dir, refine.FILE_CARDS)
    if os.path.exists(real_card_map_path):
        df_real_map = pd.read_csv(real_card_map_path, dtype=str)
        dummy_rows = []
        processed_suffixes = set()
        
        # ç‚ºäº†é¿å…åŒä¸€å¼µç¥å¡è¢«é‡è¤‡ä½¿ç”¨ï¼Œæˆ‘å€‘æ‰“äº‚æ± å­
        random.shuffle(POPULAR_CARDS_POOL)
        pool_index = 0

        for idx, row in df_real_map.iterrows():
            real_raw = str(row['å¡è™Ÿ']).strip()
            if not real_raw: continue
            
            real_suffix = real_raw[-4:]
            
            # åªè™•ç†æœ‰è¢« Himitsu å®šç¾©ï¼Œæˆ–æ˜¯æœ¬æ¬¡æœ‰æŠ½æ¨£åˆ°çš„å¡
            fake_suffix = "0000"
            if real_suffix in CUSTOM_CARD_MAP:
                fake_suffix = CUSTOM_CARD_MAP[real_suffix]
            else:
                # é€™è£¡çœ‹ä½ è¦ä¸è¦æŠŠæ‰€æœ‰çœŸå¯¦å¡è¡¨éƒ½åˆ—å‡ºä¾†ï¼Œ
                # ç‚ºäº†å±•ç¤ºè±å¯Œåº¦ï¼Œå»ºè­°å¯ä»¥éš¨æ©ŸæŠŠæ²’å®šç¾©çš„ä¹Ÿæ”¾é€²ä¾† (åªè¦ä¸æ´©æ¼çœŸå¡è™Ÿ)
                continue 

            if fake_suffix in processed_suffixes: continue
            
            # --- [æ ¸å¿ƒä¿®æ”¹] å¾ç¥å¡æ± æŠ½ä¸€å€‹èº«ä»½ ---
            if pool_index < len(POPULAR_CARDS_POOL):
                flavor = POPULAR_CARDS_POOL[pool_index]
                pool_index += 1
            else:
                # è¬ä¸€å¡ç‰‡å¤ªå¤šï¼Œæ± å­ç”¨å®Œäº†ï¼Œå°±ç”¨é€šç”¨åç¨±
                flavor = {'name': f'é€šç”¨å›é¥‹å¡_{pool_index}', 'mobile': '', 'prefix': '', 'note': 'Auto Gen'}
                pool_index += 1

            # å»ºç«‹ Mock Row
            new_row = row.copy()
            new_row['å°æ‡‰å¡ç‰‡'] = flavor['name']          # æ›¿æ›æˆç¥å¡åç¨±
            new_row['å¡è™Ÿ'] = f"**** {fake_suffix}"       # å‡è™Ÿç¢¼
            new_row['è¡Œå‹•æ”¯ä»˜æ¨™ç±¤'] = flavor['mobile']     # å¥—ç”¨è©²ç¥å¡çš„å¸¸è¦‹è¨­å®š
            new_row['åŠ åœ¨æ¶ˆè²»æ˜ç´°æ‘˜è¦å‰æ–¹'] = flavor['prefix']
            new_row['å¡è™Ÿä»£æ›'] = fake_suffix
            new_row['å‚™è¨»'] = f"[Mock] {flavor['note']}"  # æ¨™è¨»é€™æ˜¯æ¨¡æ“¬è³‡æ–™
            
            dummy_rows.append(new_row)
            processed_suffixes.add(fake_suffix)

        # é˜²å‘†ï¼šå¦‚æœéƒ½æ²’è³‡æ–™ (ä¾‹å¦‚æ²’è¨­å®š Himitsu)ï¼Œè‡³å°‘ç”Ÿå…©ç­†çµ¦äººå®¶çœ‹
        while not dummy_rows and pool_index < 2:
             flavor = POPULAR_CARDS_POOL[pool_index]
             new_row = {
                 'å°æ‡‰å¡ç‰‡': flavor['name'],
                 'å¡è™Ÿ': f"**** {1000 + pool_index}",
                 'è¡Œå‹•æ”¯ä»˜æ¨™ç±¤': flavor['mobile'],
                 'åŠ åœ¨æ¶ˆè²»æ˜ç´°æ‘˜è¦å‰æ–¹': flavor['prefix'],
                 'å¡è™Ÿä»£æ›': str(1000 + pool_index),
                 'å‚™è¨»': '[Mock] Auto Generated Demo'
             }
             dummy_rows.append(new_row)
             pool_index += 1

        # å¯«å…¥æª”æ¡ˆ
        pd.DataFrame(dummy_rows).to_csv(
            os.path.join(output_config_dir, refine.FILE_CARDS), 
            index=False, encoding='utf-8-sig'
        )

    # 2. è¤‡è£½å…¶ä»–è¨­å®šæª”
    # é€™äº›é€šå¸¸ä¸å«å€‹è³‡ï¼Œå¯ä»¥ç›´æ¥è¤‡è£½ï¼Œæˆ–éæ¿¾æ‰ Priority ä½çš„ç§æœ‰è¦å‰‡
    files_to_copy = [
        refine.FILE_CHANNELS,          # payment_gateway.csv
        refine.FILE_EXCLUDED_TYPES,        # transaction_types.yaml
        refine.FILE_EXAMPLE_MERCHANTS, # example_merchants.csv

    ]
    
    # å¦‚æœä½¿ç”¨è€…æœ¬åœ°åªæœ‰çœŸå¯¦æª”ï¼Œæ²’æœ‰ç¯„æœ¬æª” (é›–ç„¶ç…§ä½ çš„è¨ˆç•«æ˜¯æœƒæœ‰)ï¼Œæˆ‘å€‘å¯ä»¥åšå€‹ fallback
    if not os.path.exists(os.path.join(real_config_dir, 'example_merchant_regex_rules.csv')):
         # å¦‚æœæ²’æœ‰ç¯„æœ¬ï¼Œåªå¥½æš«æ™‚æ‹¿çœŸå¯¦æª” (é€™è¡Œè¦–ä½ çš„è³‡å®‰æ½”ç™–ç¨‹åº¦æ±ºå®šè¦ä¸è¦åŠ )
         # files_to_copy.append('merchant_regex_rules.csv')
         pass
    
    for f in files_to_copy:
        src = os.path.join(real_config_dir, f)
        if os.path.exists(src):
            # å¦‚æœæ˜¯ç¯„æœ¬æª” (example_merchants.csv)ï¼Œè¤‡è£½éå»æ™‚è¦æ”¹å›æ­£å¼åç¨± (merchants.csv)
            # é€™æ¨£ Mock ç’°å¢ƒçš„ç¨‹å¼æ‰èƒ½è®€åˆ°
            dst_filename = f
            if f == refine.FILE_EXAMPLE_MERCHANTS:
                dst_filename = refine.FILE_MERCHANTS
                
            shutil.copy(src, os.path.join(output_config_dir, dst_filename))
            print(f"  - Copied: {f} -> {dst_filename}")

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
def main():
    ensure_dir(OUTPUT_DIR)
    
    # 1. è®€å–çœŸå¯¦è³‡æ–™
    if not os.path.exists(SOURCE_FILE):
        print("âŒ æ‰¾ä¸åˆ°ä¾†æºè³‡æ–™ï¼Œè«‹å…ˆåŸ·è¡Œ etl.py")
        return

    print(f"1. è®€å–è³‡æ–™: {SOURCE_FILE}")
    df_raw = pd.read_csv(SOURCE_FILE, dtype=str) # å…¨éƒ¨è®€ç‚ºå­—ä¸²ä»¥å…å‹æ…‹è·‘æ‰
    
    # 2. æ¡æ¨£
    print("2. æ™ºæ…§æ¡æ¨£...")
    df_sample = smart_sample(df_raw, n=SAMPLE_SIZE)
    
    # 3. å»æ•
    print("3. å»æ•åŒ–...")
    df_masked, card_map = anonymize_data(df_sample.copy())
    
    # è¼¸å‡º Raw Example
    raw_out_path = os.path.join(OUTPUT_DIR, 'example_raw_data.csv')
    df_masked.to_csv(raw_out_path, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²è¼¸å‡º Raw Example: {raw_out_path}")
    
    # 4. æº–å‚™ Mock Configs (ç‚ºäº†è®“ refine.py å¯ä»¥è·‘)
    # æˆ‘å€‘åœ¨ examples/configs å»ºç«‹ä¸€å¥—å‡çš„è¨­å®šæª”
    mock_config_dir = os.path.join(OUTPUT_DIR, 'configs')
    generate_dummy_configs('configs', mock_config_dir, card_map)
    
    # 5. å‘¼å« Refine é‚è¼¯ç”Ÿæˆå°ç…§çµ„
    print("4. åŸ·è¡Œ Refine (ä½¿ç”¨ Mock Configs)...")
    
    # é€™è£¡æœ‰å€‹æŠ€å·§ï¼šæˆ‘å€‘æš«æ™‚æ¬ºé¨™ refine.py é—œæ–¼ Config çš„è·¯å¾‘
    # æˆ–è€…æˆ‘å€‘ç›´æ¥å‚³å…¥ dataframe çµ¦ refine çš„å‡½å¼è™•ç† (å› ç‚º refine.py å¯«å¾—å¾ˆæ¨¡çµ„åŒ–!)
    
    # è¼‰å…¥å‰›å‰›ç”Ÿæˆçš„ Mock Configs
    mock_payment_rules = refine.load_payment_rules(mock_config_dir)
    mock_merchant_rules = refine.load_merchant_regex_rules(mock_config_dir)
    mock_mapping_config = refine.load_yaml_config(os.path.join(mock_config_dir, 'mapping_rules.yaml'))
    
    # é–‹å§‹ä¸²æ¥ Refine æµç¨‹ (è¤‡è£½ refine.main çš„é‚è¼¯ï¼Œä½†æ”¹ç”¨è®Šæ•¸å‚³é)
    df_refined = df_masked.copy()
    
    # å‹æ…‹è½‰æ› (åƒè€ƒ refine.py)
    for col in ['Currency_Amount', 'Payment_Amount']:
        if col in df_refined.columns:
            df_refined[col] = pd.to_numeric(df_refined[col], errors='coerce')
            
    # åŸ·è¡Œå„éšæ®µæ¸…æ´—
    # æ³¨æ„ï¼šå› ç‚ºæˆ‘å€‘ç”Ÿæˆçš„ dummy card mapping å¯èƒ½å°ä¸ä¸Šé€™è£¡çš„å‡å¡è™Ÿï¼Œ
    # æ‰€ä»¥ apply_card_mapping æ•ˆæœå¯èƒ½æœ‰é™ï¼Œé€™åè€Œæ˜¯å¥½äº‹ (å±•ç¤ºæœªæ­¸æˆ¶ç‹€æ…‹ vs å·²æ­¸æˆ¶)
    df_refined = refine.apply_card_mapping(df_refined, config_dir=mock_config_dir)
    df_refined = refine.cleanup_cathay_remaining(df_refined)
    df_refined = refine.identify_third_party_payment(df_refined, payment_rules=mock_payment_rules)
    df_refined = refine.process_esun_epoint(df_refined)
    df_refined = refine.clean_merchant_by_regex(df_refined, regex_rules=mock_merchant_rules)
    df_refined = refine.classify_transaction_type(df_refined, mock_mapping_config)
    df_refined = refine.apply_final_prefixes(df_refined)
    
    # è¼¸å‡º Refined Example
    refined_out_path = os.path.join(OUTPUT_DIR, 'example_refined_data.csv')
    df_refined.to_csv(refined_out_path, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²è¼¸å‡º Refined Example: {refined_out_path}")

    # 6. ç”Ÿæˆ README çš„ Markdown è¡¨æ ¼ (Optional)
    # é€™åŠŸèƒ½è¶…å¯¦ç”¨ï¼Œç›´æ¥ print å‡ºä¾†è®“ä½ è²¼åˆ° GitHub README
    print("\n=== GitHub README è¡¨æ ¼é è¦½ ===")
    cols_to_show = ['Merchant', 'Merchant_Location', 'Transaction_Type', 'Payment_Amount']
    print("| åŸå§‹å•†å®¶ (Raw) | æ¸…æ´—å¾Œ (Refined) | äº¤æ˜“é¡å‹ | é‡‘é¡ |")
    print("| :--- | :--- | :--- | ---: |")
    
    # æŒ‘ 5 ç­†å±•ç¤º
    comparison = pd.DataFrame({
        'Raw': df_masked['Merchant'],
        'Refined': df_refined['Merchant'],
        'Type': df_refined['Transaction_Type'],
        'Amt': df_refined['Payment_Amount']
    }).head(5)
    
    for _, row in comparison.iterrows():
        print(f"| `{str(row['Raw'])[:15]}...` | `{row['Refined']}` | {row['Type']} | {row['Amt']} |")

if __name__ == "__main__":
    main()